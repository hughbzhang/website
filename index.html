<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Hugh Zhang</title>
  <meta name="author" content="Hugh Zhang">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
  <style>
        img {
            max-width: 100%;
            height: auto;
        }
    </style>
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Hugh Zhang</name>
              </p>
              <p>I recently decided to take a leave of absence from Harvard to join <a href="https://scale.com/"> Scale AI </a> and kickstart our open source AI research efforts.
              <br><br>
              At Harvard, I'm a PhD candidate advised by <a href="https://parkes.seas.harvard.edu/"> David Parkes </a> and supported by the <a href="https://www.nsfgrfp.org/"> NSF Graduate Research Fellowships Program </a> and a <a href="https://kempner.harvard.edu/"> Kempner Institute Graduate Fellowship</a>. Previously, I was a software engineer at <a href="https://asana.com/"> Asana </a> (gap year before college), studied Economics at Stanford, and have worked at <a href="https://research.google/teams/brain/"> Google Brain</a>, <a href="https://ai.facebook.com"> Meta AI </a>, and <a href="https://deepmind.google/"> Google Deepmind</a>. 

              My current research interest revolves around teaching large language models to do reasoning and planning. Previously, I did similar work as part of the <a href="https://ai.facebook.com/research/cicero/"> <em>CICERO</em> </a> project, the first AI agent to achieve human-level performance in the game of <a href="https://en.wikipedia.org/wiki/Diplomacy_(game)"> Diplomacy </a>.
              <br><br>
              In my spare time, I've been a lifelong Go player (in fact, seeing <a href="https://www.nature.com/articles/nature16961"> AlphaGo </a> beat Lee Sedol was origin of my interest in AI). I also co-founded the <a href="https://twitter.com/gradientpub"> Gradient </a> , a digital magazine focusing on AI.
              </p>
              <p style="text-align:center">
           
                <a href="mailto:hughbzhang@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=CgZ9uJkAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/hughbzhang">Twitter</a> &nbsp/&nbsp
                <a href="https://www.goodreads.com/user/show/60478481-hugh-zhang">Goodreads</a> &nbsp/&nbsp
                <a href="https://github.com/hughbzhang/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/HughZhang.jpg" aria-label="Profile photo"><img style="width:100%;max-width:100%" alt="profile photo" src="images/PROFILE.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My current work focuses on evals, test-time compute, and post-training for LLMs. Previously, I also worked on multi-agent reinforcement learning and game theory. <strong>* denotes equal or alphabetical ordering.</strong>

              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div style="display:flex;gap:5px">
                <img src="images/o1_original.jpg" alt="o1_scaling" style="width:50%">
                <img src="images/accuracy_vs_tokens_no_shade_regions.png" alt="accuracy_vs_tokens" style="width:50%">
              </div>
            </td>
            <td width="75%" valign="middle">
              <a href="https://github.com/hughbzhang/o1_inference_scaling_laws">
                <papertitle>Reconstructing O1 Test-Time Compute Scaling Laws</papertitle>
              </a>
              <br>
              <strong>Hugh Zhang</strong>, Celia Chen
              <br>
              <a href="https://x.com/hughbzhang/status/1838288923656941860">twitter</a>
              <br>
              <p>Reconstructed o1 test-time scaling laws using public API access to o1-mini.
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/plansearch.jpg" alt="plansearch">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2409.03733">
                <papertitle>Planning In Natural Language Improves LLM Search For Code Generation</papertitle>
              </a>
              <br>
              Evan Wang, Federico Cassano, Catherine Wu, Yunfeng Bai, Will Song, Vaskar Nath, Ziwen Han, Sean Hendryx, Summer Yue, <strong>Hugh Zhang</strong>
              <br>
              <a href="https://x.com/hughbzhang/status/1832079839840575705">twitter</a> /
              <a href="https://x.com/alexandr_wang/status/1832147956562284987">twitter2</a>
              <br>
              <p>Searching over a diverse set of ideas/plans in natural language significantly helps code generation and is far more effective that repeated sampling.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mhj.png" alt="mhj">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2408.15221">
                <papertitle>LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet</papertitle>
              </a>
              <br>
              Nathaniel Li, Ziwen Han, Ian Steneker, Willow Primack, Riley Goodside, <strong>Hugh Zhang</strong>, Zifan Wang, Cristina Menghini, Summer Yue
              <br>
              <em>Red Teaming GenAI Workshop @ NeurIPS</em>, 2024
              <br>
              <a href="https://x.com/natliml/status/1828476257186783674">twitter</a>
              <br>
              <p>We demonstrate that multi-turn human jailbreaks can achieve >70% success rates against LLM defenses that report single-digit success rates for automated single-turn attacks.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/gsm1k.jpg" alt="gsm1k">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2405.00332">
                <papertitle>A Careful Examination of Large Language Model Performance on Grade School Arithmetic</papertitle>
              </a>
              <br>
              <strong>Hugh Zhang</strong>, Jeff Da, Dean Lee, Vaughn Robinson, Catherine Wu, Will Song, Tiffany Zhao, Pranav Raja, Dylan Slack, Qin Lyu, Sean Hendryx, Russell Kaplan, Michele Lunati, Summer Yue
              <br>
              <em>NeurIPS Spotlight (Datasets and Benchmarks Track)</em>, 2023
              <br>
              <a href="https://exchange.scale.com/public/videos/unmasking-llm-performance-how-to-benchmark-the-reasoning-abilities-of-llms-2024-09-04">talk</a> /
              <a href="https://docs.google.com/presentation/d/18FTC2w4VousOHsFuMsiWJ87LufoBRS-tN8Jutrr0GQg/edit?usp=sharing">slides</a> /
              <a href="https://x.com/hughbzhang/status/1785877026794356858">twitter</a> /
              <a href="https://x.com/polynoamial/status/1785864074678714520">twitter2</a> /
              <a href="https://x.com/karpathy/status/1795873666481402010">twitter3</a> /
              <a href="https://x.com/alexandr_wang/status/1785888203943161970">twitter4</a>
              <br>
              <p>We clone GSM8k to measure dataset contamination. Some models show signs of overfitting, but frontier models show strong generalization.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/goal_conditioned.png" alt="gcrl">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2407.13887">
                <papertitle>Learning Goal-Conditioned Representations for Language Reward Models</papertitle>
              </a>
              <br>
              Vaskar Nath, Dylan Slack, Jeff Da, Yuntao Ma, <strong>Hugh Zhang</strong>, Spencer Whitehead, Sean Hendryx
              <br>
              <em>NeurIPS</em>, 2024
              <p> Representation learning may be useful for post-training LLMs.
              </p>
              <br>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/qprobe.jpg" alt="QProbe">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2402.14688">
                <papertitle>Q-Probe: A Lightweight Approach to Reward Maximization for Language Models</papertitle>
              </a>
              <br>
              <a href="https://likenneth.github.io/">Kenneth Li</a>, 
              <a href="https://sjelassi.github.io/">Samy Jelassi</a>,
              <strong>Hugh Zhang</strong>,
              <a href="https://sham.seas.harvard.edu/">Sham Kakade</a>,
              <a href="https://www.bewitched.com/">Martin Wattenberg</a>,
              <a href="https://davidbrandfonbrener.github.io/">David Brandfonbrener</a>
              
              <br>
              <p>A lightweight alternative to fine-tuning that performs better than LORA for very small datasets and requires minimal model access.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/SECToR.png" alt="SECToR">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2309.08589">
                <papertitle>Chain-of-Thought Reasoning is a Policy Improvement Operator</papertitle>
              </a>
              <br>
              <strong>Hugh Zhang</strong>, <a href="https://parkes.seas.harvard.edu/">David C. Parkes</a> 
              <br>
              <em>Workshop on Instruction Tuning and Instruction Following at NeurIPS 2023</em>
              <br>
              <a href="https://twitter.com/hughbzhang/status/1728834567241617861">twitter</a> /
              <a href="https://docs.google.com/presentation/d/1imBiG2oqYrWrvX471nI6VR0QaWl_rD6mPpFAhLtOTYg/edit#slide=id.g230e609b8a2_0_38">slides </a> /
              <a href="https://docs.google.com/presentation/d/1lzyGtYl2bG2cvfHm2qwvV7A-ZUWAKrCIx2lnVLy_X_k/edit#slide=id.g10e10bc6bc4_0_39">poster </a>
              <br>
              <p>Training on chain-of-thoughts that lead to a correct answer can help a LLM self-improve and generalize far beyond their original capabilities in the toy environment of addition.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/abcs.png" alt="ABCs" width="100%" maxwidth="160px" height="100%">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2402.11835">
                <papertitle>Easy as ABCs: Unifying Boltzmann Q-Learning and Counterfactual Regret Minimization</papertitle>
              </a>
              <br> 
              <a href="https://scholar.google.com/citations?user=RqF7N0cAAAAJ&hl=en">Luca D'Amico-Wong*</a> , <strong>Hugh Zhang*</strong>, <a href="https://scholar.google.com/citations?user=E_oZZj8AAAAJ&hl=en">Marc Lanctot</a> , <a href="https://parkes.seas.harvard.edu/">David C. Parkes</a> 
              <br>
              <a href="https://github.com/lucadwong/abcs">code </a>
              <br>
              <p>Unified algorithm for both reinforcement learning and game theory. Can solve MDPs as fast as RL methods and imperfect-information games as fast as CFR using the <i>single set of hyperparameters.</i></p>
            </td>
          </tr>

          <tr onmouseout="diplomacy_stop()" onmouseover="diplomacy_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='diplomacy_image'>
                  <video  width=100% muted autoplay loop>
                    <source src="images/DIPLOMACY.mp4" width="160" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
                <img src='images/DIPLOMACY.jpg' width="160">
              </div>
              <script type="text/javascript">
                function diplomacy_start() {
                  document.getElementById('diplomacy_image').style.opacity = "1";
                }

                function diplomacy_stop() {
                  document.getElementById('diplomacy_image').style.opacity = "0";
                }
                diplomacy_stop()
              </script>
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.science.org/doi/10.1126/science.ade9097?fbclid=IwAR2Z3yQJ1lDMuBUyfICtHnWz2zRZEhbodBkAJlYshvxkCqpcYFhq5a_Cg6Q">
                <papertitle>Human-Level Play In The Game Of Diplomacy By Combining Language Models With Strategic Reasoning</papertitle>
              </a>
              <br>
              Anton Bakhtin*, Noam Brown*, Emily Dinan*, Gabriele Farina*, Colin Flaherty*, Daniel Fried*, Andrew Goff*, Jonathan Gray*, Hengyuan Hu*, Athul Paul Jacob*, Mojtaba Komeili*, Karthik Konath*, Adam Lerer*, Mike Lewis*, Alexander H. Miller*, Sasha Mitts*, Adithya Renduchintala*, Stephen Roller*, Dirk Rowe*, Weiyan Shi*, Joe Spisak*, Alexander Wei*, David Wu*, <strong>Hugh Zhang*</strong>, Markus Zijlstra*
              <br>
              <em>Science</em>, 2022
              <br>
              <a href="data/diplomacy.pdf">paper</a>
              /
              <a href="https://ai.facebook.com/blog/cicero-ai-negotiates-persuades-and-cooperates-with-people/?utm_source=twitter&utm_medium=organic_social&utm_campaign=cicero&utm_content=video">blog</a>
              /
              <a href="https://www.nytimes.com/2023/01/20/technology/chatbots-turing-test.html"> nyt </a>
              /
              <a href="https://www.economist.com/science-and-technology/2022/11/23/another-game-falls-to-an-ai-player"> economist </a>
              /
              <a href="https://gizmodo.com/meta-ai-cicero-diplomacy-gaming-1849811840"> gizmodo </a>
              /
              <a href="https://www.forbes.com/sites/carlieporterfield/2022/11/22/metas-ai-gamer-beat-humans-in-diplomacy-using-strategy-and-negotiation/"> forbes </a> 
              /
              <a href="https://www.newscientist.com/article/2348152-metas-board-game-playing-ai-can-pass-as-a-human-in-game-negotiations/"> new scientist </a> 
              /
              <a href="https://arstechnica.com/information-technology/2022/11/meta-researchers-create-ai-that-masters-diplomacy-tricking-human-players/"> ars technica </a> 
              /
              <a href="https://www.technologyreview.com/2022/11/23/1063648/metas-game-playing-ai-can-make-and-break-alliances-like-a-human/">mit tech review </a> 
              /
              <a href="https://kotaku.com/facebook-meta-ai-artificial-intelligence-board-game-win-1849815368">kotaku</a> 
              /
              <a href="https://www.engadget.com/add-diplomacy-to-the-list-of-activities-ai-can-do-as-well-as-humans-140024906.html">engadget</a> 
              /
              <a href="https://www.theregister.com/2022/11/23/metas_cicero_chatbot_can_probably/">register </a> 
              /
              <a href="https://news.ycombinator.com/item?id=33706750"> hacker news </a>
              /
              <a href="https://www.reddit.com/r/MachineLearning/comments/z1yt45/r_humanlevel_play_in_the_game_of_diplomacy_by/"> reddit </a>
              <p></p>
              <p>
              Human level performance in the game of Diplomacy, where agents negotiate with other humans in natural language.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/GREEDY_WEIGHTS.jpg" alt="GREEDY_WEIGHTS" width="100%" maxwidth="160px" height="100%">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2204.04826">
                <papertitle>Equilibrium Finding in Normal-Form Games Via Greedy Regret Minimization</papertitle>
              </a>
              <br>
              <strong>Hugh Zhang</strong>, <a href="https://scholar.google.com/citations?user=Ad6O4-0AAAAJ&hl=en">Adam Lerer</a>, <a href="https://www.cs.cmu.edu/~noamb/">Noam Brown</a>
              <br>
              <em>Association for the Advancement of Artificial Intelligence (AAAI)</em>, 2022
              <p>A novel no-regret learning procedure that converges to correlated and coarse-correlated equilibria several orders of magnitude faster than previous methods in randomly generated normal-form games.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/LIKELIHOOD_TRAP.jpg" alt="LIKELIHOOD_TRAP" width="100%" maxwidth="160px" height="100%">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2004.10450">
                <papertitle>Trading Off Diversity and Quality in Natural Language Generation</papertitle>
              </a>
              <br>
              <strong>Hugh Zhang*</strong>, <a href="https://www.stronglyconvex.com/about.html">Daniel Duckworth*</a>, <a href="https://www.seas.upenn.edu/~daphnei/me/">Daphne Ippolito</a>, <a href="https://scholar.google.com/citations?user=ygTCc6cAAAAJ&hl=en">Arvind Neelakantan</a> 
              <br>
              <em>Workshop on Human Evaluation of Natural Language Processing Systems at the Conference of the European Chapter of the Association for Computational Linguistics (HumEval Workshop @EACL)</em>, 2021
              <p>The first large-scale evaluation of decoding methods for large language models along the entire quality-diversity spectrum.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/FORGIVING_CORRELATED_EQUILIBRIA.jpg" alt="FORGIVING_CORRELATED_EQUILIBRIA" width="100%" maxwidth="160px" height="100%">
            </td>
            <td width="75%" valign="middle">
              <a href="https://economics.stanford.edu/simple-adaptive-procedure-converging-forgiving-correlated-equilibria">
                <papertitle>A Simple Adaptive Procedure Converging to Forgiving Correlated Equilibria</papertitle>
              </a>
              <br>
              <strong>Hugh Zhang</strong> (advised by <a href="http://individual.utoronto.ca/carroll/">Gabriel Carroll</a>)
              <br>
              <em>Stanford Senior Honors Thesis in Economics</em>, 2020 
              <strong>(John G. Sobieski Award for Creative Thinking)</strong>
              <p>Alongside <a href="https://arxiv.org/abs/2004.00603"> Celli et. al (2020) </a> (concurrent work), this paper gives the first internal regret minimization dynamics for extensive-form games.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/HUSE.jpg" alt="HUSE" width="100%" maxwidth="160px" height="100%">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1904.02792">
                <papertitle>Unifying Human and Statistical Evaluation for Natural Language Generation</papertitle>
              </a>
              <br>
              <a href="https://thashim.github.io/">Tatsunori Hashimoto* </a>, <strong>Hugh Zhang*</strong>, <a href="https://cs.stanford.edu/~pliang/">Percy Liang </a>
              <br>
              <em>North American Chapter of the Association for Computational Linguistics (NAACL)</em>, 2019  <strong>(Oral Presentation)</strong>
              <p>Existing language models can generate either high quality or diverse utterances, but not both simultaneously. How can we measure that in a single metric?</p>
            </td>
          </tr>

        </tbody></table>

				
        
					
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Thanks to <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a> for this website's template.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
